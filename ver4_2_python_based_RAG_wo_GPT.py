import pandas as pd
import re
import unicodedata
from typing import Dict, List, Tuple

#â€•â€•â€•â€•â€•â€•â€•â€•â€•â€• è¨­å®š â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•#
CSV_PATH = r"C:\Users\takeda\Documents\projectRAG\restructured_file.csv"
MAX_DISPLAY_ROWS = 30

cols = ["ä½œæ¥­å", "ä¸‹åœ°ã®çŠ¶æ³", "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•",
        "ãƒ©ã‚¤ãƒŠãƒƒã‚¯ã‚¹æ©Ÿç¨®å", "ä½¿ç”¨ã‚«ãƒƒã‚¿ãƒ¼å", "å·¥ç¨‹æ•°", "ä½œæ¥­åŠ¹ç‡è©•ä¾¡"]

FILTER_COLUMNS = {
    "ä½œæ¥­å": ["ä½œæ¥­å"],
    "ä¸‹åœ°ã®çŠ¶æ³": ["ä¸‹åœ°ã®çŠ¶æ³"],
    "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•": ["å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•"],
    "æ©Ÿæ¢°ã‚«ãƒ†ã‚´ãƒªãƒ¼": ["æ©Ÿæ¢°ã‚«ãƒ†ã‚´ãƒªãƒ¼"],
    "ãƒ©ã‚¤ãƒŠãƒƒã‚¯ã‚¹æ©Ÿç¨®å": ["ãƒ©ã‚¤ãƒŠãƒƒã‚¯ã‚¹æ©Ÿç¨®å"],
    "ä½¿ç”¨ã‚«ãƒƒã‚¿ãƒ¼å": ["ä½¿ç”¨ã‚«ãƒƒã‚¿ãƒ¼å"],
    "ä½œæ¥­åŠ¹ç‡è©•ä¾¡": ["ä½œæ¥­åŠ¹ç‡è©•ä¾¡"],
}

CATEGORY_NUM_MAP = {
    "1": "ä½œæ¥­å",
    "2": "ä¸‹åœ°ã®çŠ¶æ³",
    "3": "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•",
    "4": "æ©Ÿæ¢°ã‚«ãƒ†ã‚´ãƒªãƒ¼",
    "5": "ãƒ©ã‚¤ãƒŠãƒƒã‚¯ã‚¹æ©Ÿç¨®å",
    "6": "ä½¿ç”¨ã‚«ãƒƒã‚¿ãƒ¼å",
    "7": "ä½œæ¥­åŠ¹ç‡è©•ä¾¡",
}

#â€•â€•â€•â€•â€•â€•â€•â€•â€•â€• ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•#
def normalize_text(s: str) -> str:
    if pd.isna(s):
        return ""
    s = str(s)
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"[ã€ã€‚ãƒ»ï¼Œ,]", " ", s)
    return s.strip().lower()

def load_data(path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
    raw_df = pd.read_csv(path, dtype=str).fillna("")
    norm_df = raw_df.copy()
    for c in raw_df.columns:
        norm_df[c] = raw_df[c].apply(normalize_text)
    return raw_df, norm_df

def build_unique_dict(norm_df: pd.DataFrame) -> Dict[str, List[str]]:
    uniq: Dict[str, List[str]] = {}
    for cat, columns in FILTER_COLUMNS.items():
        values = pd.concat([norm_df[c] for c in columns])
        uniq[cat] = sorted(set(values.values) - {""})
    return uniq

def split_keywords(text: str) -> List[str]:
    text = normalize_text(text)
    return re.split(r"\s+", text)

def expand_synonyms(keywords: List[str], synonym_dict: Dict[str, List[str]]) -> List[str]:
    expanded = []
    for word in keywords:
        expanded.append(word)
        expanded.extend(synonym_dict.get(word, []))
    return list(set(expanded))

def build_auto_synonyms(norm_df: pd.DataFrame, target_columns: List[str], min_len: int = 2) -> Dict[str, List[str]]:
    keyword_map = {}
    values = set()
    for col in target_columns:
        values.update(norm_df[col].unique())
    for val in values:
        for other in values:
            if val == other:
                continue
            if len(val) >= min_len and val in other:
                keyword_map.setdefault(val, []).append(other)
    return keyword_map

def build_known_keywords(df: pd.DataFrame, min_length: int = 2) -> List[str]:
    unique_words = set()
    for col in df.columns:
        for val in df[col]:
            val = str(val)
            val = re.sub(r"[ï¼ˆï¼‰()ã€Œã€ã€ã€‘\[\]ã€ˆã€‰]", " ", val)
            tokens = re.split(r"[,\u3001\u3002\u3000\s]+", val)
            for t in tokens:
                t_norm = normalize_text(t)
                if len(t_norm) >= min_length and re.search(r'\w', t_norm):
                    unique_words.add(t_norm)
    return sorted(unique_words)

def select_category_by_number() -> str:
    print("\nã‚«ãƒ†ã‚´ãƒªã‚’ç•ªå·ã§é¸ã‚“ã§ãã ã•ã„ï¼ˆEnterã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰:")
    for num, name in CATEGORY_NUM_MAP.items():
        print(f"[{num}] {name}")
    choice = normalize_text(input("> ").strip())
    if not choice:
        return ""
    return CATEGORY_NUM_MAP.get(choice, "")

def parse_depth_range(text: str) -> Tuple[float, float]:
    match = re.search(r"([0-9.]+)\s*-\s*([0-9.]+)", text)
    if match:
        return float(match.group(1)), float(match.group(2))
    return (None, None)

def is_value_in_range(value: float, text_range: str) -> bool:
    low, high = parse_depth_range(text_range)
    if low is None or high is None:
        return False
    return low <= value <= high

def suggest_filters(
    uniq: Dict[str, List[str]],
    query: str,
    known_keywords: List[str],
    synonym_dict: Dict[str, List[str]]
) -> Dict[str, List[str]]:
    suggestions: Dict[str, List[str]] = {cat: [] for cat in FILTER_COLUMNS}
    query_keywords = split_keywords(query)
    expanded_keywords = expand_synonyms(query_keywords, synonym_dict)

    matched_terms = [word for word in known_keywords if any(eq in word for eq in expanded_keywords)]
    print("\n[DEBUG] æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å€™è£œ:", matched_terms)

    depth_match = re.findall(r"[\d\\.]+", query)
    if depth_match:
        try:
            val = float(depth_match[0])
            print("[DEBUG] æ•°å€¤ã«ã‚ˆã‚‹å‡¦ç†æ·±ã•ã®æŠ½å‡º:", val)
            suggestions["å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•"].append(str(val))
        except:
            pass

    for word in matched_terms:
        for cat, values in uniq.items():
            if cat == "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•":
                continue
            matches = [v for v in values if word in v]
            suggestions[cat].extend(matches)

    return suggestions

def filter_data(norm_df: pd.DataFrame, filters: Dict[str, List[str]]) -> pd.DataFrame:
    mask = pd.Series(True, index=norm_df.index)
    for cat, terms in filters.items():
        if not terms:
            continue
        if cat == "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•":
            try:
                target_value = float(terms[0])
                match_mask = norm_df["å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•"].apply(
                    lambda x: is_value_in_range(target_value, x)
                )
                mask &= match_mask
            except Exception as e:
                print(f"[WARN] æ·±ã•ã®åˆ¤å®šã§ã‚¨ãƒ©ãƒ¼: {e.__class__.__name__} - {e}")
                mask &= False
        else:
            pat = "|".join(re.escape(t) for t in terms)
            sub = pd.Series(False, index=norm_df.index)
            for col in FILTER_COLUMNS.get(cat, []):
                sub |= norm_df[col].str.contains(pat, na=False)
            mask &= sub
    return norm_df[mask]

def extract_engineering_pairs(filtered_hits: pd.DataFrame, full_df: pd.DataFrame) -> List[pd.DataFrame]:
    seen = set()
    pairs = []
    candidate_keys = [
        (row["ä½œæ¥­å"], row["ä¸‹åœ°ã®çŠ¶æ³"], row["å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•"])
        for _, row in filtered_hits.iterrows()
        if row["å·¥ç¨‹æ•°"] in ["ä¸€æ¬¡å·¥ç¨‹", "äºŒæ¬¡å·¥ç¨‹"]
    ]
    for key in candidate_keys:
        if key in seen:
            continue
        seen.add(key)
        step1 = full_df[
            (full_df["ä½œæ¥­å"] == key[0]) &
            (full_df["ä¸‹åœ°ã®çŠ¶æ³"] == key[1]) &
            (full_df["å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•"] == key[2]) &
            (full_df["å·¥ç¨‹æ•°"] == "ä¸€æ¬¡å·¥ç¨‹")
        ]
        step2 = full_df[
            (full_df["ä½œæ¥­å"] == key[0]) &
            (full_df["ä¸‹åœ°ã®çŠ¶æ³"] == key[1]) &
            (full_df["å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•"] == key[2]) &
            (full_df["å·¥ç¨‹æ•°"] == "äºŒæ¬¡å·¥ç¨‹")
        ]
        if not step1.empty and not step2.empty:
            pairs.append(pd.concat([step1, step2]))
    return pairs

def print_pair_results(pairs: List[pd.DataFrame]):
    def sort(df):
        df = df.copy()
        order = {"â—": 0, "ã€‡": 1, "â—‹": 1, "â–³": 2}
        df["_å·¥ç¨‹é †"] = df["å·¥ç¨‹æ•°"].map(lambda x: 0 if "ä¸€æ¬¡" in x else 1 if "äºŒæ¬¡" in x else 2)
        df["_åŠ¹ç‡é †"] = df["ä½œæ¥­åŠ¹ç‡è©•ä¾¡"].map(lambda x: order.get(x.strip(), 99))
        return df.sort_values(["_å·¥ç¨‹é †", "_åŠ¹ç‡é †"]).drop(columns=["_å·¥ç¨‹é †", "_åŠ¹ç‡é †"])
    if pairs:
        print("\n===ï¼ˆä¸€æ¬¡å·¥ç¨‹ï¼‹äºŒæ¬¡å·¥ç¨‹ãƒšã‚¢ï¼‰===")
        for i, pair in enumerate(pairs, 1):
            print(f"\n--- ãƒšã‚¢ {i} ---")
            print(sort(pair)[cols].to_string(index=False))

def print_solo_results(df: pd.DataFrame, exclude_keys: List[Tuple[str, str, str]]):
    def sort(df):
        df = df.copy()
        order = {"â—": 0, "ã€‡": 1, "â—‹": 1, "â–³": 2}
        df["_å·¥ç¨‹é †"] = df["å·¥ç¨‹æ•°"].map(lambda x: 0 if "ä¸€æ¬¡" in x else 1 if "äºŒæ¬¡" in x else 2)
        df["_åŠ¹ç‡é †"] = df["ä½œæ¥­åŠ¹ç‡è©•ä¾¡"].map(lambda x: order.get(x.strip(), 99))
        return df.sort_values(["_å·¥ç¨‹é †", "_åŠ¹ç‡é †"]).drop(columns=["_å·¥ç¨‹é †", "_åŠ¹ç‡é †"])
    df = df[~df.apply(lambda r: (r["ä½œæ¥­å"], r["ä¸‹åœ°ã®çŠ¶æ³"], r["å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•"]) in exclude_keys, axis=1)]
    if not df.empty:
        print("\n=== å˜ä¸€å·¥ç¨‹ãƒ»ãã®ä»–ã®å·¥æ³•çµæœ ===\n")
        print(sort(df)[cols].to_string(index=False))

def print_row(row):
    print(f"{row['ä½œæ¥­å']: <12} {row['ä¸‹åœ°ã®çŠ¶æ³']: <20} {row['å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•']: <15} "
          f"{row['ãƒ©ã‚¤ãƒŠãƒƒã‚¯ã‚¹æ©Ÿç¨®å']: <10} {row['ä½¿ç”¨ã‚«ãƒƒã‚¿ãƒ¼å']: <15} {row['å·¥ç¨‹æ•°']: <8} {row['ä½œæ¥­åŠ¹ç‡è©•ä¾¡']}")

def summarize_and_print(results, raw_df):
    if results.empty:
        print("\nè©²å½“ã™ã‚‹å·¥æ³•ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    efficiency_order = {"â—": 0, "â—‹": 1, "ã€‡": 1, "â–³": 2}

    # å˜ä¸€å·¥ç¨‹
    singles = results[results["å·¥ç¨‹æ•°"] == "å˜ä¸€"]

    # ä¸€æ¬¡ or äºŒæ¬¡å·¥ç¨‹ãŒãƒ’ãƒƒãƒˆã—ãŸè¡Œ
    multi_hits = results[results["å·¥ç¨‹æ•°"] != "å˜ä¸€"]

    # === ãƒšã‚¢è£œå®Œ ===
    pairs = []
    seen_keys = set()
    for _, row in multi_hits.iterrows():
        key = (row["ä½œæ¥­å"], row["ä¸‹åœ°ã®çŠ¶æ³"], row["å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•"])
        if key in seen_keys:
            continue
        seen_keys.add(key)

        step1 = raw_df[
            (raw_df["ä½œæ¥­å"] == key[0]) &
            (raw_df["ä¸‹åœ°ã®çŠ¶æ³"] == key[1]) &
            (raw_df["å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•"] == key[2]) &
            (raw_df["å·¥ç¨‹æ•°"] == "ä¸€æ¬¡å·¥ç¨‹")
        ]
        step2 = raw_df[
            (raw_df["ä½œæ¥­å"] == key[0]) &
            (raw_df["ä¸‹åœ°ã®çŠ¶æ³"] == key[1]) &
            (raw_df["å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•"] == key[2]) &
            (raw_df["å·¥ç¨‹æ•°"] == "äºŒæ¬¡å·¥ç¨‹")
        ]

        if not step1.empty or not step2.empty:
            # å·¥ç¨‹ã”ã¨ã«â—â†’â—‹â†’â–³ã‚½ãƒ¼ãƒˆ
            step1_sorted = step1.sort_values(
                by="ä½œæ¥­åŠ¹ç‡è©•ä¾¡", key=lambda col: col.map(efficiency_order)
            )
            step2_sorted = step2.sort_values(
                by="ä½œæ¥­åŠ¹ç‡è©•ä¾¡", key=lambda col: col.map(efficiency_order)
            )
            pairs.append(pd.concat([step1_sorted, step2_sorted]))

    # === å˜ä¸€å·¥ç¨‹è¡¨ç¤ºï¼ˆâ—â†’â—‹â†’â–³ï¼‰ ===
    if not singles.empty:
        singles_sorted = singles.sort_values(
            by="ä½œæ¥­åŠ¹ç‡è©•ä¾¡", key=lambda col: col.map(efficiency_order)
        )
        print("\n=== å˜ä¸€å·¥ç¨‹ãƒ»ãã®ä»–ã®å·¥æ³•çµæœ ===\n")
        print(singles_sorted[
            ["ä½œæ¥­å", "ä¸‹åœ°ã®çŠ¶æ³", "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•",
             "ãƒ©ã‚¤ãƒŠãƒƒã‚¯ã‚¹æ©Ÿç¨®å", "ä½¿ç”¨ã‚«ãƒƒã‚¿ãƒ¼å", "å·¥ç¨‹æ•°", "ä½œæ¥­åŠ¹ç‡è©•ä¾¡"]
        ].to_string(index=False))

    # === ãƒšã‚¢è¡¨ç¤ºï¼ˆä¸€æ¬¡å·¥ç¨‹â†’äºŒæ¬¡å·¥ç¨‹ã€å·¥ç¨‹å†…ã§â—â†’â—‹â†’â–³ï¼‰ ===
    if pairs:
        print("\n===ï¼ˆä¸€æ¬¡å·¥ç¨‹ï¼‹äºŒæ¬¡å·¥ç¨‹ãƒšã‚¢ï¼‰===\n")
        for i, pair in enumerate(pairs, 1):
            print(f"--- ãƒšã‚¢ {i} ---")
            print(pair[
                ["ä½œæ¥­å", "ä¸‹åœ°ã®çŠ¶æ³", "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•",
                 "ãƒ©ã‚¤ãƒŠãƒƒã‚¯ã‚¹æ©Ÿç¨®å", "ä½¿ç”¨ã‚«ãƒƒã‚¿ãƒ¼å", "å·¥ç¨‹æ•°", "ä½œæ¥­åŠ¹ç‡è©•ä¾¡"]
            ].to_string(index=False))
            print()

    print(f"=== ç·è©²å½“ä»¶æ•°: {len(results)} ä»¶ ===\n")

#â€•â€•â€•â€•â€•â€•â€•â€•â€•â€• ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼ â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•#

def main():
    raw_df, norm_df = load_data(CSV_PATH)
    uniq = build_unique_dict(norm_df)
    known_keywords = build_known_keywords(raw_df)

    # âœ… é¡ç¾©èªå±•é–‹ã®å¯¾è±¡åˆ—ã‚’æ‹¡å¼µï¼ˆ6åˆ—ã™ã¹ã¦ï¼‰
    synonym_dict = build_auto_synonyms(norm_df, [
        "ä½œæ¥­å", "ä¸‹åœ°ã®çŠ¶æ³", "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•",
        "ãƒ©ã‚¤ãƒŠãƒƒã‚¯ã‚¹æ©Ÿç¨®å", "ä½¿ç”¨ã‚«ãƒƒã‚¿ãƒ¼å", "å·¥ç¨‹æ•°"
    ])

    filters: Dict[str, List[str]] = {cat: [] for cat in FILTER_COLUMNS}

    while True:
        query = input("\næŠ½å‡ºã—ãŸã„å·¥æ³•ã¾ãŸã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„> ").strip()
        if not query:
            print("å…¥åŠ›ãªã—ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
            break

        suggestions = suggest_filters(uniq, query, known_keywords, synonym_dict)
        for cat in FILTER_COLUMNS:
            filters[cat].extend(suggestions.get(cat, []))

        while True:
            print("\n--- ç¾åœ¨ã®ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ ---")
            for cat, terms in filters.items():
                display = ["æœªæŒ‡å®š"]
                if terms:
                    if cat == "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•":
                        try:
                            target_value = float(terms[0])

                            # âœ… ä½œæ¥­å or ä¸‹åœ°ã®çŠ¶æ³ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã‘ã‚Œã°å‡¦ç†ã—ãªã„
                            if not filters["ä½œæ¥­å"] and not filters["ä¸‹åœ°ã®çŠ¶æ³"]:
                                display = ["æœªæŒ‡å®šï¼ˆä½œæ¥­åã¾ãŸã¯ä¸‹åœ°ã®çŠ¶æ³ã®æŒ‡å®šãŒå¿…è¦ï¼‰"]
                                filters[cat] = []
                            else:
                                sub_df = norm_df

                                if filters["ä½œæ¥­å"]:
                                    work_pat = "|".join(re.escape(t) for t in filters["ä½œæ¥­å"])
                                    sub_df = sub_df[sub_df["ä½œæ¥­å"].str.contains(work_pat, na=False)]

                                if filters["ä¸‹åœ°ã®çŠ¶æ³"]:
                                    base_pat = "|".join(re.escape(t) for t in filters["ä¸‹åœ°ã®çŠ¶æ³"])
                                    sub_df = sub_df[sub_df["ä¸‹åœ°ã®çŠ¶æ³"].str.contains(base_pat, na=False)]

                                matched_ranges = sorted({
                                    raw_df.at[i, "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•"]
                                    for i in sub_df.index
                                    if is_value_in_range(target_value, norm_df.at[i, "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•"])
                                })
                                display = matched_ranges if matched_ranges else ["æœªæŒ‡å®šï¼ˆè©²å½“ãªã—ï¼‰"]
                                if display == ["æœªæŒ‡å®šï¼ˆè©²å½“ãªã—ï¼‰"]:
                                    filters[cat] = []
                        except Exception as e:
                            display = [f"è§£æã‚¨ãƒ©ãƒ¼: {e.__class__.__name__} - {e}"]
                    else:
                        matched_values = set()
                        for col in FILTER_COLUMNS.get(cat, []):
                            for t in terms:
                                matched_values |= set(
                                    raw_df[col][norm_df[col].str.contains(re.escape(t), na=False)].unique()
                                )
                        display = sorted(matched_values) if matched_values else ["æœªæŒ‡å®šï¼ˆè©²å½“ãªã—ï¼‰"]
                        if display == ["æœªæŒ‡å®šï¼ˆè©²å½“ãªã—ï¼‰"]:
                            filters[cat] = []

                print(f"{cat}: {', '.join(display)}")

            ans = normalize_text(input(
                "\nã“ã®æ¡ä»¶ã§æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ "
                "(1=å®Ÿè¡Œ / 2=ã‚­ãƒ£ãƒ³ã‚»ãƒ« / 3=ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ  / 4=æ¡ä»¶ã‚¯ãƒªã‚¢ / 5=æ¡ä»¶ä¸€éƒ¨ä¿®æ­£)> "
            ))

            if ans in ['y', '1']:
                hits_norm = filter_data(norm_df, filters)
                hits_raw = raw_df.loc[hits_norm.index]

                if hits_raw.empty:
                    print("è©²å½“ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    if "å·¥ç¨‹æ•°" not in hits_raw.columns:
                        hits_raw = hits_raw.merge(
                            raw_df[["ä½œæ¥­ID", "å·¥ç¨‹æ•°"]],
                            on="ä½œæ¥­ID",
                            how="left"
                        )
                    summarize_and_print(hits_raw, raw_df)

                again = normalize_text(input("\næ¡ä»¶ã‚’å¤‰æ›´ã—ã¦å†æ¤œç´¢ã—ã¾ã™ã‹ï¼Ÿ (y/n)> "))
                if again != 'y':
                    print("çµ‚äº†ã—ã¾ã™ã€‚")
                    return
                else:
                    print("\nğŸ” å‰å›ã®ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ç¶­æŒã—ã¦å†æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
                    continue

            elif ans in ['n', '2']:
                print("æ¤œç´¢ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚")
                break

            elif ans in ['a', '3']:
                cat_add = select_category_by_number()
                if cat_add:
                    extra = input(f"{cat_add} ã«è¿½åŠ ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›ï¼ˆEnterã§ã‚­ãƒ£ãƒ³ã‚»ãƒ«ï¼‰> ").strip()
                    if not extra:
                        print(f"{cat_add} ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
                        continue
                    new_terms = []
                    for t in extra.split(','):
                        t_norm = normalize_text(t)
                        if not t_norm:
                            continue
                        if cat_add == "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•":
                            match = re.search(r"[\d.]+", t_norm)
                            if match:
                                new_terms.append(match.group(0))
                            else:
                                print(f"ç„¡åŠ¹ãªæ·±ã•æŒ‡å®š: {t}")
                        else:
                            new_terms.append(t_norm)
                    filters[cat_add].extend(new_terms)
                else:
                    print("ã‚«ãƒ†ã‚´ãƒªå…¥åŠ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

            elif ans in ['c', '4']:
                filters = {cat: [] for cat in FILTER_COLUMNS}
                print("ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ã™ã¹ã¦ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚æ–°ãŸã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                break

            elif ans in ['m', '5']:
                cat_mod = select_category_by_number()
                if cat_mod:
                    current_terms = filters.get(cat_mod, [])
                    if not current_terms:
                        print(f"{cat_mod} ã¯ç¾åœ¨ æœªæŒ‡å®š ã§ã™ã€‚")
                    else:
                        print(f"\n{cat_mod} ã®ç¾åœ¨ã®æ¡ä»¶: {', '.join(current_terms)}")

                    print("1 = æ¡ä»¶ã‚’ä¸Šæ›¸ã / 2 = æ¡ä»¶ã‚’å‰Šé™¤ï¼ˆæœªæŒ‡å®šã«æˆ»ã™ï¼‰ / 3 = ã‚­ãƒ£ãƒ³ã‚»ãƒ«")
                    action = normalize_text(input("> ").strip())
                    if action == '1':
                        new_input = input(f"{cat_mod} ã®æ–°ã—ã„å€¤ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›> ").strip()
                        new_terms = []
                        for t in new_input.split(','):
                            t_norm = normalize_text(t)
                            if not t_norm:
                                continue
                            if cat_mod == "å‡¦ç†ã™ã‚‹æ·±ã•ãƒ»åšã•":
                                match = re.search(r"[\d.]+", t_norm)
                                if match:
                                    new_terms.append(match.group(0))
                                else:
                                    print(f"ç„¡åŠ¹ãªæ·±ã•æŒ‡å®š: {t}")
                            else:
                                new_terms.append(t_norm)
                        filters[cat_mod] = new_terms
                        print(f"{cat_mod} ã®æ¡ä»¶ã‚’ä¸Šæ›¸ãã—ã¾ã—ãŸã€‚")
                    elif action == '2':
                        filters[cat_mod] = []
                        print(f"{cat_mod} ã®æ¡ä»¶ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                    elif action == '3':
                        print("å¤‰æ›´ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚")
                    else:
                        print("ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚")
                else:
                    print("ã‚«ãƒ†ã‚´ãƒªå…¥åŠ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

            else:
                print("1ã€œ5 ã¾ãŸã¯ y, n, a, c, m ã®ã„ãšã‚Œã‹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")



if __name__ == '__main__':
    main()